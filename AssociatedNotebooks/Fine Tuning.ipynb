{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "We are going to try and fine tune a FCN classifier so to segment cells from histopathology scans. Our implementation is based on the implementation based on FCN.  \n",
    "\n",
    "\n",
    "The fine tuning wil be addressed in four steps. Preparing the data in the form of blobs, so as to be caffee compatible but also in three model training. We will train with a 32 stride, take the parameters and plug them into a finer 16 stride model, and finally the same protocole is used to go a 8 stride finner model.\n",
    "\n",
    "We will use the pre-train models available in the caffee zoo. We will remove each final layer and fine tune it via back propagation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preparing the data\n",
    "\n",
    "Under the library caffee, data is provided to model under the form of blobs. A blob is a 4 dimensinal array where each channels represents: (num, channel, height, width). We will be providing cell segmentation data from histopathology slides. \n",
    "It can accept several types: HDF5, LMDB, etc..\n",
    "Lets create an LMDB file to feed the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naylor/anaconda/lib/python2.7/site-packages/skimage/util/dtype.py:110: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "from DataToLMDB import MakeLMDB\n",
    "import ImageTransf as Transf\n",
    "\n",
    "path = '/home/naylor/Bureau/ToAnnotate'\n",
    "#path = \"/data/users/pnaylor/Bureau/ToAnnotate\"\n",
    "wd = '/home/naylor/Documents/Python/PhD/PhD_Fabien/AssociatedNotebooks'\n",
    "\n",
    "output_dir = path + '/lmdb'\n",
    "\n",
    "enlarge = False ## create symetry if the image becomes black ? \n",
    "\n",
    "transform_list = [Transf.Identity(),\n",
    "                  Transf.Rotation(45, enlarge=enlarge), \n",
    "                  Transf.Rotation(90, enlarge=enlarge),\n",
    "                  Transf.Rotation(135, enlarge=enlarge),\n",
    "                  Transf.Flip(0),\n",
    "                  Transf.Flip(1),\n",
    "                  Transf.OutOfFocus(5),\n",
    "                  Transf.OutOfFocus(10),\n",
    "                  Transf.ElasticDeformation(0, 30, num_points = 4),\n",
    "                  Transf.ElasticDeformation(0, 30, num_points = 4)]\n",
    "\n",
    "mean_ = MakeLMDB(path, output_dir, transform_list, val_num = 2, get_mean=True, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 243.14176483  195.61314077  172.87945147]\n"
     ]
    }
   ],
   "source": [
    "print mean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the 32 stride model: Pascal voc example\n",
    "\n",
    "The architecture and pre-trained weight parameters can be found on the FCN github page. The architecture is based on the famous AlexNet implementation, the authors performed \"net surgery\" to transform the fully connected layers (final layers of the AlexNet model) into fully convolutionnal layers. These layers have the particularity of accepting any given size of inputs. Once these layers \"convoloutionnized\" , they added an upsampling and a deconvolution layer as to improve the accuracy of the model, indeed, AlexNet models provided the \"what\" but does not initially provide the where. Bridges were also added in order to combine low lever features to higher level features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import caffe\n",
    "import os\n",
    "\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "\n",
    "weights =  '/home/naylor/Documents/FCN/fcn.berkeleyvision.org/voc-fcn32s/fcn32s-heavy-pascal.caffemodel'\n",
    "assert os.path.exists(weights)\n",
    "solver = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the architecture right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "from caffe import layers as L, params as P\n",
    "from caffe.coord_map import crop\n",
    "\n",
    "def conv_relu(bottom, nout, ks=3, stride=1, pad=1):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "        num_output=nout, pad=pad,\n",
    "        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "def max_pool(bottom, ks=2, stride=2):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "\n",
    "def fcn32(batch_size, lmdb, mean_, layer_name = \"\"):\n",
    "    n = caffe.NetSpec()\n",
    "\n",
    "    n.data, n.label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2,\n",
    "        transform_param=dict(mean_value=list(mean_), scale = 1/256., mirror=True))\n",
    "    # the base net\n",
    "\n",
    "    n.conv1_1, n.relu1_1 = conv_relu(n.data, 64, pad=100)\n",
    "    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 64)\n",
    "    n.pool1 = max_pool(n.relu1_2)\n",
    "\n",
    "    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 128)\n",
    "    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 128)\n",
    "    n.pool2 = max_pool(n.relu2_2)\n",
    "\n",
    "    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 256)\n",
    "    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 256)\n",
    "    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 256)\n",
    "    n.pool3 = max_pool(n.relu3_3)\n",
    "\n",
    "    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 512)\n",
    "    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 512)\n",
    "    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 512)\n",
    "    n.pool4 = max_pool(n.relu4_3)\n",
    "\n",
    "    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 512)\n",
    "    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 512)\n",
    "    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 512)\n",
    "    n.pool5 = max_pool(n.relu5_3)\n",
    "\n",
    "    # fully conv\n",
    "    n.fc6, n.relu6 = conv_relu(n.pool5, 4096, ks=7, pad=0)\n",
    "    \n",
    "    n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True)\n",
    "    \n",
    "    n.fc7, n.relu7 = conv_relu(n.drop6, 4096, ks=1, pad=0)\n",
    "    \n",
    "    n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True)\n",
    "    \n",
    "    n.score_fr = L.Convolution(n.drop7, num_output=2, kernel_size=1, pad=0,\n",
    "        param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])\n",
    "    \n",
    "    n.upscore = L.Deconvolution(n.score_fr,\n",
    "        convolution_param=dict(num_output=2, kernel_size=64, stride=32,\n",
    "            bias_term=False),\n",
    "        param=[dict(lr_mult=0)])\n",
    "    \n",
    "    n.score = crop(n.upscore, n.data)\n",
    "    \n",
    "    n.loss = L.SoftmaxWithLoss(n.score, n.label,\n",
    "            loss_param=dict(normalize=False, ignore_label=255))\n",
    "\n",
    "    return n.to_proto()\n",
    "\n",
    "def make_net(output_dir):\n",
    "    if not os.path.isdir('./fcn32'):\n",
    "        os.mkdir('./fcn32')\n",
    "    with open('fcn32/train.prototxt', 'w') as f:\n",
    "        f.write(str(fcn32(32, output_dir, mean_)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    make_net(output_dir)\n",
    "    # load the solver\n",
    "    #solver = caffe.SGDSolver('fcn32/train.prototxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting the solver\n",
    "Writes a file that will have all the parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function layer_fn in module caffe.net_spec:\n",
      "\n",
      "layer_fn(*args, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def solver(train_net_path, test_net_path=None, base_lr=0.001, out_snap=\"./temp_snapshot\"):\n",
    "    s = caffe_pb2.SolverParameter()\n",
    "    s.train_net = train_net_path\n",
    "    if test_net_path is not None:\n",
    "        s.test_net.append(test_net_path)\n",
    "        s.test_interval = 1000  # Test after every 1000 training iterations.\n",
    "        s.test_iter.append(100) # Test on 100 batches each time we test.\n",
    "\n",
    "    # The number of iterations over which to average the gradient.\n",
    "    # Effectively boosts the training batch size by the given factor, without\n",
    "    # affecting memory utilization.\n",
    "    s.iter_size = 1\n",
    "    \n",
    "    s.max_iter = 100000     # # of times to update the net (training iterations)\n",
    "    \n",
    "    # Solve using the stochastic gradient descent (SGD) algorithm.\n",
    "    # Other choices include 'Adam' and 'RMSProp'.\n",
    "    s.type = 'SGD'\n",
    "\n",
    "    # Set the initial learning rate for SGD.\n",
    "    s.base_lr = base_lr\n",
    "\n",
    "    # Set `lr_policy` to define how the learning rate changes during training.\n",
    "    # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
    "    # every `stepsize` iterations.\n",
    "    s.lr_policy = 'step'\n",
    "    s.gamma = 0.1\n",
    "    s.stepsize = 20000\n",
    "\n",
    "    # Set other SGD hyperparameters. Setting a non-zero `momentum` takes a\n",
    "    # weighted average of the current gradient and previous gradients to make\n",
    "    # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
    "    # the model from overfitting.\n",
    "    s.momentum = 0.9\n",
    "    s.weight_decay = 5e-4\n",
    "\n",
    "    # Display the current training loss and accuracy every 1000 iterations.\n",
    "    s.display = 1000\n",
    "\n",
    "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
    "    # snapshot every 10K iterations -- ten times during training.\n",
    "    s.snapshot = 10000\n",
    "    if not os.path.isdir(out_snap):\n",
    "        os.mkdir(out_snap)\n",
    "    s.snapshot_prefix = out_snap\n",
    "    \n",
    "    # Train on the GPU.  Using the CPU to train large networks is very slow.\n",
    "    s.solver_mode = caffe_pb2.SolverParameter.GPU\n",
    "    \n",
    "    # Write the solver to a temporary file and return its filename.\n",
    "    with open('fcn32/train.prototxt', 'w') as f:\n",
    "        f.write(str(s))\n",
    "        return f.name\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
